[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Here’s a short introduction about me:\n\n\nI’m currently working as a junior IT & Data Consultant somewhere in the very pretty business hub of La Défense, but I’m also trying to learn a bit more about programming and web development. I’m a self-anointed pythonista 🐍, but I’m trying to learn Julia & Javascript aswell !\nI also work a lot with SQL at my current job, though I’m eyeing on the recent trends like Snowflake and BigQuery 👀.\nHere’s a picture of my workplace (and some others I took in the area):\n\n\n\n\n\n\nI’m currently working on a few projects, but I’m not sure I’ll be able to finish them anytime soon. I’m also trying to learn a bit more about web development, so I’m trying to make a few websites. Here’s a list of the projects I’m currently working on:\n\n\n\n\n\n\n\n\nNumber\nLanguage\nDescription\n\n\n\n\n1\n🐍\nA few unpolished python packages\n\n\n2\n🐍\nA python data API for eurostat related data\n\n\n3\nJS\nA Vue.js website to slowly learn Vue.js\n\n\n4\n🤓\nA small blog about math related topics when I feel like talking theory\n\n\n\nThat’s a lot ! But I also have some personnal matters …"
  },
  {
    "objectID": "about.html#im-autistic-queer",
    "href": "about.html#im-autistic-queer",
    "title": "About",
    "section": "I’m Autistic & Queer",
    "text": "I’m Autistic & Queer\nI’m a proud Aspie / Neurodivergent person, I received an official diagnosis from Hôpital Saint-Anne in Paris, France a year or so ago. They’re considered a bit competent on the topic so I trust them.\n\n\n\nI’m also most probably trans 🏳️‍⚧️, but I’m not sure 100% yet. I’m currently trying to figure out how to go about it, the process is a bit complicated in France, but it’s a work in progress.\nI wrote a short piece about the relationship between autism and gender diversity a while back."
  },
  {
    "objectID": "about.html#i-like-games",
    "href": "about.html#i-like-games",
    "title": "About",
    "section": "I like games",
    "text": "I like games\nI’m also a big video game player, so maybe we can play together sometime ! I’m currently a bit busy with work, but I’d love to share some factoids about my favorite games."
  },
  {
    "objectID": "about.html#i-like-music",
    "href": "about.html#i-like-music",
    "title": "About",
    "section": "I like music",
    "text": "I like music\nI’m a HUGE music nerd, just to name a few of my favorite genres:\n\n\n\n\n\n\n\nGenre\nDescription\n\n\n\n\n70’s rock\nespecially progressive rock and Cantebury scene / Krautrock bands\n\n\nBebop and Hard-Bop\nThe golden age of Bebop and Hard-Bop, especially the work of Miles Davis, who helped me bridge the gap between Jazz and Rock (I’m a huge fan of his work with John McLaughlin)\n\n\nIDM\nEspecially the work of μ-Ziq, Floating Points, Squarepusher, and Boards of Canada\n\n\nHip-Hop\nThe classics, especially the work of MF Doom, Madlib, and J Dilla\n\n\nHyperpop\nI’m a huge fan of the work of Charli XCX !\n\n\nK-pop\nI’ve recently discovered I like K-pop thanks to League of Legends’ KDA group, and I’m slowly getting into the genre ..\n\n\nMetal\nI love Gojira obviously, but a lot of artists here and there, the progressive Metal scene is pretty dope aswell 🤩\n\n\n\n… and many more 🎵"
  },
  {
    "objectID": "about.html#i-like-math",
    "href": "about.html#i-like-math",
    "title": "About",
    "section": "I like math",
    "text": "I like math\nI’m also a big math nerd, I’m currently trying to learn a bit more about statistics and probability theory, as I’m a recovering tentative PhD grad (I worked on Causal Inference back in Shanghai).\nI’d love learning more, but I’m busy and already have a lot to do !"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arno’s Data Science snippets",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nHierachical modelling in Julia (basics) 🧑‍🏫\n\n\n\n\n\n\n\njulia\n\n\ncode\n\n\nanalysis\n\n\neconomics\n\n\nhierachical-models\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nArno V\n\n\n\n\n\n\n  \n\n\n\n\nJumping into curve fitting ! 🚀\n\n\n\n\n\n\n\njulia\n\n\ncode\n\n\nanalysis\n\n\neconomics\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\nArno V\n\n\n\n\n\n\n  \n\n\n\n\nFirst attempt at modelling using Julia\n\n\n\n\n\n\n\njulia\n\n\ncode\n\n\nanalysis\n\n\neconomics\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\nArno V\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to my data notebooks blog !\n\n\n\n\n\n\n\nlaunch\n\n\nblog\n\n\nhello\n\n\ncode\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nArno V\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/distribution-fitting/index.html",
    "href": "posts/distribution-fitting/index.html",
    "title": "Jumping into curve fitting ! 🚀",
    "section": "",
    "text": "Last time we were just looking at the relative income distribution. This time we’ll try to fit some curves to it.\nThe goal is to find a parametric model \\(P_{\\phi}\\) that fits the data well for some \\(\\phi\\) (we’ll most probably work with \\(\\mathbb{R}\\)-valued vector \\(\\phi\\)’s).\nNow though I have made a clean CSV file with the simplified data, let me just grab that 😉\n\n\nShow the code\nusing DataFrames, DataFramesMeta, BrowseTables, CSV\nusing Distributions, StatsPlots, Statistics\nusing Plots\nusing LaTeXStrings\n\ndf = DataFrame(CSV.File(\"C:\\\\Users\\\\arnov\\\\Documents\\\\code\\\\notebooks\\\\quarto\\\\econ\\\\data\\\\data-clean-full-latest-x-y-cdf.csv\"))\n\nscatter(\n    df.perc_mean,\n    df.rel_income,\n    label=L\"Data points $(p, i_p)$\",\n    xlabel=L\"Percentile $p$\",\n    ylabel=L\"Relative income $i_p$\",\n    size=(800, 600),\n    background_color=\"#7711d708\",\n    markersize=10,\n    markerstrokewidth=2,\n    markerstrokecolor=\"black\",\n    markeralpha=0.2,\n    markercolor=:pink,\n    legendfontsize=18,\n    guidefontsize=18,\n)\n\n\n\n\n\n\n\n\nLet’s start with the most basic model, any of the positive parametric distributions.\nWe’ve got three choices I can think of:\n\nThe Gamma distribution \\(\\Gamma(\\alpha, \\beta)\\) with \\(\\phi = (\\alpha, \\beta)\\)\nThe LogNormal distribution \\(\\mathcal{LN}(\\mu, \\sigma)\\) with \\(\\phi = (\\mu, \\sigma)\\)\nThe Weibull distribution \\(\\mathcal{W}(\\lambda, k)\\) with \\(\\phi = (\\lambda, k)\\)\n\nWe’re not going to think really hard for now, we’ll just use the most basic method for fitting the curve:\n\ndismiss that those are quantiles\nuse them as plain points to perform likelihood maximization\n\n\n\nImportant to note that the Distributions.jl package implements the Gamma using this parametrization:\n\\[ f(x) = \\frac{1}{\\Gamma(\\alpha) \\theta^\\alpha} x^{\\alpha - 1} e^{-\\frac{x}{\\theta}}\\qquad\\text{for } x > 0 \\]\nThis is a small detail worth nothing 🤓. The interface to fit is the same for all: you get your data \\(\\vec{x} \\coloneqq [x_1, \\ldots, x_n]\\) into an array and you call fit_mle(Distribution, data).\n\n\nShow the code\nvalidPts = filter(x -> x .> 0, df.rel_income)\nfittedGamma = fit_mle(Gamma, validPts)\nymax = maximum(pdf.(fittedGamma,validPts))\n\n\n\nplot(\n    fittedGamma,\n    label=L\"Gamma $\\Gamma$ fit\",\n    xlabel=L\"Relative income $i_p$\",\n    ylabel=L\"Probability density $f(i_p)$\",\n    size=(800, 600),\n    color=:pink,\n    fill=(0, 0.2, :pink),\n    background_color=\"#7711d708\",\n    legendfontsize=18,\n    guidefontsize=18,\n    ylims=(0, ymax*1.1),\n    title=\"Gamma fit to the data, \\$k = $(round(fittedGamma.α, sigdigits=3)), \\\\theta = $(round(fittedGamma.θ, sigdigits=3))\\$\",\n)\n\n\n\n\n\n\n\nShow the code\nusing Markdown\n\nNLL = -loglikelihood(fittedGamma, validPts)\n\nls = \"\"\"\n    Alright that's pretty good, we've got our \\$\\\\Gamma\\$ fit to the data, with\n    \\$k = $(round(fittedGamma.α, sigdigits=3)),\n    \\\\theta = $(round(fittedGamma.θ, sigdigits=3))\\$\n    \n    How good is this fit though ?  \n    Let's check the log-likelihood of the data under this model. \n    \n    \n    \\$\\\\text{log}P_{\\\\phi}(\\\\vec{x}) = $(round(NLL, sigdigits=3))\\$\n    \n    \"\"\"\nMarkdown.parse(ls)\n\n\nAlright that’s pretty good, we’ve got our \\(\\Gamma\\) fit to the data, with \\(k = 1.83, \\theta = 14.5\\)\nHow good is this fit though ? Let’s check the log-likelihood of the data under this model.\n\\[\n\\text{log}P_{\\phi}(\\vec{x}) = 2070.0\n\\]\n\n\n\nIt’s been a while since I actually looked at NLL values but this seems quite high. Let’s check the QQ plot to see how it looks.\n\n\nShow the code\nqqplot(fittedGamma, validPts,\nlabel=L\"$\\Gamma$ fit\", xlabel=L\"Theoretical Quantiles $q$\", \nylabel=L\"Observed quantiles: Relative income $i_p$\", \nsize=(800, 600),\nbackground_color=\"#7711d708\",\nlegendfontsize=18,\nguidefontsize=18,\ncolor=:pink,\nmarkersize=5,\nmarkerstrokewidth=1,\nmarkerstrokecolor=\"purple\",\nmarkeralpha=0.3,\nmarkercolor=:pink,)\n\n\n\n\n\nUh oh, that’s not good. The QQ plot is not looking good at all.\nOut of curiosity lets see the result of the KS test.\n\n\nShow the code\nusing HypothesisTests\n\nks = ExactOneSampleKSTest(validPts, fittedGamma)\n\nls = \"\"\"\n    The likelihood that the data is drawn from the fitted Gamma distribution is\n\n    given by the \\$p\\$-value \\$D_n(d)\\$ where \\$d = \\\\lVert F_n - F \\\\rVert_{\\\\infty}\\$,\n\n    here:\n    \\$p = $(round(pvalue(ks, tail= :both), sigdigits=3))\\$\n    \"\"\"\nMarkdown.parse(ls)\n\n\nThe likelihood that the data is drawn from the fitted Gamma distribution is\ngiven by the \\(p\\)-value \\(D_n(d)\\) where \\(d = \\lVert F_n - F \\rVert_{\\infty}\\),\nhere: \\(p = 8.24e-6\\)\n\n\n\nWell that’s a way to put it. The p-value is on the order of \\(10^{-6}\\), OK. In comparison most scientific papers use a threshold for rejection of the null that’s \\(\\approx 10^{-2}\\). We’re roughly \\(10000\\) more confident here ! Let’s try the other two.\n\n\n\nThe LogNormal is just a gaussian squished through a logarithmic post-processor, it’s a bit more intuitive but there’s no reason it should be better than the Gamma (a priori).\n\n\nShow the code\nfittedLogNormal = fit_mle(LogNormal, validPts)\n\nymax = maximum(pdf.(fittedLogNormal, validPts))\n\nplot(\n    fittedLogNormal,\n    label=L\"LogNormal $\\mathcal{LN}$ fit\",\n    xlabel=L\"Relative income $i_p$\",\n    ylabel=L\"Probability density $f(i_p)$\",\n    size=(800, 600),\n    color=:pink,\n    fill=(0, 0.2, :pink),\n    background_color=\"#7711d708\",\n    legendfontsize=18,\n    ylims=(0, ymax*1.1),\n    guidefontsize=18,\n    title=\"LogNormal fit to the data, \\$\\\\mu = $(round(fittedLogNormal.μ, sigdigits=3)), \\\\sigma = $(round(fittedLogNormal.σ, sigdigits=3))\\$\",\n)\n\n\n\n\n\n\nAlright, let’s not have our hopes up too high. Let’s check the QQ plot.\n\n\nShow the code\nqqplot(fittedLogNormal, validPts,\nlabel=L\"$\\mathcal{LN}$ fit\", xlabel=L\"Theoretical Quantiles $q$\",\nylabel=L\"Observed quantiles: Relative income $i_p$\",\nsize=(800, 600),\nbackground_color=\"#7711d708\",\nlegendfontsize=18,\nguidefontsize=18,\ncolor=:pink,\nmarkersize=5,\nmarkerstrokewidth=1,\nmarkerstrokecolor=\"purple\",\nmarkeralpha=0.3,\nmarkercolor=:pink,)\n\n\n\n\n\n\nThis is hilariously bad !! No use checking the KS test.\nLast but not least, the Weibull distribution.\n\n\n\nWeibull is a more general version of the exponential distribution, a \\(W\\) weibull variable is generally \\(W \\propto (X/\\lambda)^k\\) where \\(X\\) is a \\(\\lambda\\)-scale exponential variable.\n\n\nShow the code\nfittedWeibull = fit_mle(Weibull, validPts)\n\nymax = maximum(pdf.(fittedWeibull, validPts ))\n\nplot(\n    fittedWeibull,\n    label=L\"Weibull $\\mathcal{W}$ fit\",\n    xlabel=L\"Relative income $i_p$\",\n    ylabel=L\"Probability density $f(i_p)$\",\n    size=(800, 600),\n    color=:pink,\n    fill=(0, 0.2, :pink),\n    background_color=\"#7711d708\",\n    legendfontsize=18,\n    guidefontsize=18,\n    ylims=(0, ymax*1.1),\n    title=\"Weibull fit to the data, \\$\\\\alpha = $(round(fittedWeibull.α, sigdigits=3)), \\\\beta = $(round(fittedWeibull.θ, sigdigits=3))\\$\",\n)\n\n\n\n\n\n\nLet’s check the QQ plot.\n\n\nShow the code\nqqplot(fittedWeibull, validPts,\nlabel=L\"$\\mathcal{W}$ fit\", xlabel=L\"Theoretical Quantiles $q$\",\nylabel=L\"Observed quantiles: Relative income $i_p$\",\nsize=(800, 600),\nbackground_color=\"#7711d708\",\nlegendfontsize=18,\nguidefontsize=18,\ncolor=:pink,\nmarkersize=5,\nmarkerstrokewidth=1,\nmarkerstrokecolor=\"purple\",\nmarkeralpha=0.3,\nmarkercolor=:pink,)\n\n\n\n\n\n\nOoohh 👀 hey there, that’s not bad at all. Let’s check the NLL first.\n\n\nShow the code\nNLL = -loglikelihood(fittedWeibull, validPts)\n\nls = \"\"\"\n    The (negative) log-likelihood of the data under this model is..\n    \n    \\$\\\\text{log}P_{\\\\phi}(\\\\vec{x}) = $(round(NLL, sigdigits=3))\\$\n    \n    \"\"\"\nMarkdown.parse(ls)\n\n\nThe (negative) log-likelihood of the data under this model is..\n\\[\n\\text{log}P_{\\phi}(\\vec{x}) = 2050.0\n\\]\n\n\n\nHmm, not sure what to make of this. Let’s check the KS test.\n\n\nShow the code\nks = ExactOneSampleKSTest(validPts, fittedWeibull)\n\nls = \"\"\"\n    The likelihood that the data is drawn from the fitted Weibull distribution is\n\n    given by the \\$p\\$-value \\$D_n(d)\\$, which is equal to\n\n    \\$p = $(round(pvalue(ks, tail= :both), sigdigits=3))\\$\n    \"\"\"\nMarkdown.parse(ls)\n\n\nThe likelihood that the data is drawn from the fitted Weibull distribution is\ngiven by the \\(p\\)-value \\(D_n(d)\\), which is equal to\n\\[\np = 4.99e-5\n\\]\n\n\n\nMeh, all of them get zero but the KS test is known to be a bit strict.\nLet’s see if we get a bit more information using the \\(L_2\\) norm.\n\n\nShow the code\nadWeibull = OneSampleADTest(validPts, fittedWeibull)\nksWeibull = ExactOneSampleKSTest(validPts, fittedWeibull)\n\nadGamma = OneSampleADTest(validPts, fittedGamma)\nksGamma = ExactOneSampleKSTest(validPts, fittedGamma)\n\nadLogNormal = OneSampleADTest(validPts, fittedLogNormal)\nksLogNormal = ExactOneSampleKSTest(validPts, fittedLogNormal)\n\nls = \"\"\"\n    To summarize, here are the results of the tests:\n\n    | Distribution | parameters | NLL | \\$L_2\\$ norm \\$p\\$-value | \\$L_\\\\infty\\$ norm \\$p\\$-value |\n    |:------------:|:----------:|:----------:|:--------------------:|:-------------------------:|\n    | Weibull | \\$\\\\alpha = $(round(fittedWeibull.α, sigdigits=3)), \\\\theta = $(round(fittedWeibull.θ, sigdigits=3))\\$ | $(round(-loglikelihood(fittedWeibull, validPts), sigdigits=3)) | $(round(pvalue(adWeibull), sigdigits=3)) | $(round(pvalue(ksWeibull, tail= :both), sigdigits=3)) |\n    | Gamma | \\$\\\\alpha = $(round(fittedGamma.α, sigdigits=3)), \\\\theta = $(round(fittedGamma.θ, sigdigits=3))\\$ | $(round(-loglikelihood(fittedGamma, validPts), sigdigits=3)) | $(round(pvalue(adGamma), sigdigits=3)) | $(round(pvalue(ksGamma, tail= :both), sigdigits=3)) |\n    | LogNormal | \\$\\\\mu = $(round(fittedLogNormal.μ, sigdigits=3)), \\\\sigma = $(round(fittedLogNormal.σ, sigdigits=3))\\$ | $(round(-loglikelihood(fittedLogNormal, validPts), sigdigits=3)) | $(round(pvalue(adLogNormal), sigdigits=3)) | $(round(pvalue(ksLogNormal, tail= :both), sigdigits=3)) |\n    \"\"\"\n\nMarkdown.parse(ls)\n\n\nTo summarize, here are the results of the tests:\n\n\n\n\n\n\n\n\n\n\nDistribution\nparameters\nNLL\n\\(L_2\\) norm \\(p\\)-value\n\\(L_\\infty\\) norm \\(p\\)-value\n\n\n\n\nWeibull\n\\(\\alpha = 1.61, \\theta = 29.4\\)\n2050.0\n0.000188\n4.99e-5\n\n\nGamma\n\\(\\alpha = 1.83, \\theta = 14.5\\)\n2070.0\n1.41e-5\n8.24e-6\n\n\nLogNormal\n\\(\\mu = 2.98, \\sigma = 0.956\\)\n2160.0\n1.21e-6\n2.0e-8\n\n\n\n\n\nThat’s all for today folks ! We’ll try hierarchical models next time 🕵️"
  },
  {
    "objectID": "posts/first-steps-julia/index.html",
    "href": "posts/first-steps-julia/index.html",
    "title": "First attempt at modelling using Julia",
    "section": "",
    "text": "A first attempt at modelling using Julia\n✨ Economic data 📊 , Julia 🤗 and Jupyter🪐\n\n\n\nThe setup : libs & data\n\n\nShow the code\nusing DataFrames, DataFramesMeta, BrowseTables, Parquet\nusing Distributions, StatsPlots, Statistics\nusing Plots\nusing LaTeXStrings\n\n\n\nImporting the data\n\n\nShow the code\ndf = DataFrame(\n    read_parquet(\"C:\\\\Users\\\\arnov\\\\Documents\\\\code\\\\notebooks\\\\quarto\\\\econ\\\\data\\\\data-clean-full-latest.parquet\"))\n\nfirst(df, 10)\n\n\n\n10×8 DataFrameRowperccountryyearrel_incomeoriginalcodenumlabelFloat64?String?Int64?Float64?String?String?Int64?String?10.1Albania20172.0D1:First decileD1First decile20.1Albania20182.0D1:First decileD1First decile30.1Albania20192.3D1:First decileD1First decile40.1Albania20202.5D1:First decileD1First decile50.1Austria19953.0D1:First decileD1First decile60.1Austria19964.0D1:First decileD1First decile70.1Austria19974.0D1:First decileD1First decile80.1Austria19984.0D1:First decileD1First decile90.1Austria19994.0D1:First decileD1First decile100.1Austria20004.0D1:First decileD1First decile\n\n\n\n\n\nWhat we aim to do\nHere we have two values, \\(Q(p)\\) and \\(p\\), where \\(p\\) is a percentile and \\(Q\\) the Quantile function.\nYou could conversely see it as \\(q\\) and \\(F(q)\\), where \\(q\\) is a quantile and \\(F\\) the CDF.\nWe want to find a decent model for \\(F\\). We’ll try a few methods first:\n\nThe classic Q-Q plot eyeballing\nThe Kolmogorov-Smirnov test (based on the \\(L_\\infty\\) norm \\(||F-\\hat{F}||_\\infty\\))\nThe so-called Anderson-Darling test (based on the \\(L_2\\) norm \\(||F-\\hat{F}||_2\\))\n\nLet’s first look at the data: the true model actually depends on \\(\\langle t, k, p \\rangle\\) (time, country and percentile), but we’ll ignore that for now.\n\n\nShow the code\ndfg = groupby(df, :rel_income)\n\ndfr = hcat(\n    combine(dfg,\n        :perc => ((p) -> quantile(p, 0.025)) => :perc_025,\n        :perc => ((p) -> quantile(p, 0.05)) => :perc_05,\n        :perc => ((p) -> quantile(p, 0.25)) => :perc_25,\n        :perc => ((p) -> quantile(p, 0.5)) => :perc_50,\n        :perc => ((p) -> quantile(p, 0.75)) => :perc_75,\n        :perc => ((p) -> quantile(p, 0.95)) => :perc_95,\n        :perc => ((p) -> quantile(p, 0.975)) => :perc_975,\n        ),\n    combine(dfg,\n        :perc => mean => :perc_mean,\n        :perc => std => :perc_std,\n        ),\n        makeunique=true\n)\n\n\n\n506×11 DataFrame481 rows omittedRowrel_incomeperc_025perc_05perc_25perc_50perc_75perc_95perc_975rel_income_1perc_meanperc_stdFloat64?Float64Float64Float64Float64Float64Float64Float64Float64?Float64Float6412.00.10.10.950.950.970.980.982.00.8870810.23835922.30.10.440.960.970.980.990.992.30.9258740.19311832.50.10.10.970.980.980.990.992.50.9141110.22806443.00.10.10.10.980.990.990.993.00.580370.44283254.00.10.10.10.10.21.01.04.00.3277480.3660263.50.10.10.10.10.990.99951.03.50.4667740.43410373.60.10.10.10.150.991.01.03.60.4325710.4207583.80.10.10.10.21.01.01.03.80.4742310.43647193.40.10.10.10.980.991.01.03.40.5643860.440106103.30.10.10.10.10.990.9941.03.30.4915070.443608113.20.10.10.10.980.991.01.03.20.6036250.439773123.10.10.10.10.980.990.990.993.10.6617070.429311133.70.10.10.10.10.991.01.03.70.3794740.406705⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮49539.31.01.01.01.01.01.01.039.31.00.049635.81.01.01.01.01.01.01.035.81.00.049734.21.01.01.01.01.01.01.034.21.00.049837.21.01.01.01.01.01.01.037.21.00.049937.61.01.01.01.01.01.01.037.61.00.050036.11.01.01.01.01.01.01.036.11.00.050133.61.01.01.01.01.01.01.033.61.00.050233.21.01.01.01.01.01.01.033.21.00.050333.11.01.01.01.01.01.01.033.11.0NaN50434.11.01.01.01.01.01.01.034.11.0NaN50532.31.01.01.01.01.01.01.032.31.0NaN50649.51.01.01.01.01.01.01.049.51.00.0\n\n\n\n\nConfidence intervals\nLet’s estimate upper and lower confidence bounds for the CDF function \\(F\\).\n\n\nShow the code\ndfr = hcat(\n    dfr,\n    combine(dfr,\n        [:perc_mean, :perc_std] => ByRow.((m,s) -> m + 2 * s) => :perc_upper_std,\n        [:perc_mean, :perc_std] => ByRow.((m,s) -> m - 2 * s) => :perc_lower_std\n        ),\n    makeunique=true\n)\ndfr = select(dfr, Not(:rel_income_1))\n\n\nlatexstring(raw\"\"\"\n\\mu(P) = \"\"\" * \"$(round(mean(df.perc), digits=2))\" * raw\"\"\"\\quad \\text{and} \\quad\n\\sigma(P) = \"\"\" * \"$(round(std(df.perc), digits=2))\" )\n\n\n\\(\\mu(P) = 0.57\\quad \\text{and} \\quad \\sigma(P) = 0.36\\)\n\n\n\nThis means that on aggregate we have this mental picture of the typical quartile distribution:\n\n\nShow the code\nmu = round(mean(df.perc), digits=2)\n\nsigma = round(std(df.perc), digits=2)\n\nplot(\n    Normal(\n        mu,\n        sigma\n        ),\n        fill=(0, .2, :pink),\n        linecolor=:purple,\n        label=\"\"\"Normal plot \\\\mu = $(mu), \\\\sigma = $(sigma)\"\"\",\n        size=(800, 600),\n        background_color=\"#7711d708\"\n)\n\n\n\n\n\nBut that aggregate picture is not very useful. Let’s look at the distribution by relative income.\n\n\nShow the code\nfunction linspace(start::Any, stop::Any, n::Int)\n    step = (stop - start) / (n - 1)\n    return start:step:stop\nend\n\nfunction diracPlot(x, y, label = \"\")\n    plot!([x,x],[0,y],arrow=true,linewidth=2, label=label)\nend\n\nbins = linspace(minimum(df.rel_income), maximum(df.rel_income), 5)\n\ndfbin = combine(df,\n    :rel_income => ByRow.((x) -> bins[findfirst(bins .>= x)]) => :bin_rel_income,\n    :perc => identity => :perc\n    )\n\ndfgbin = combine(\n    groupby(dfbin, :bin_rel_income),\n    :perc => mean => :perc_mean,\n    :perc => std => :perc_std\n    )\n\np = plot(\n    vline(\n        [mean(df.perc)],\n        label=\"Total Mean\",\n        linestyle=:dash,\n        linewidth=2,\n        ylims=(0, 2.5),\n        legend=:outertopright,\n        background_color=\"#7711d708\"\n        ),\n)\nfor i in 1:size(dfgbin, 1)\n    sstd = dfgbin[i, :perc_std]\n    smean = dfgbin[i, :perc_mean]\n    bin_rel_inc = bins[i]\n    if isnan(sstd) || sstd <= 1e-6\n        diracPlot(smean, 2, \"Group $(i) \\\\mu = $(round(smean, digits=2)) , \\\\sigma = 0\")\n        sstd = 0\n    else\n        vline!(\n        [smean],\n        label=\"\",\n        linewidth=0.5,\n        ylims=(0, 2.5),\n        )\n        plot!(\n        Normal(\n            smean,\n            sstd\n            ),\n            fill=(0, .2),\n            label=\"Group $(i) Normal \\\\mu = $(round(smean, digits=2)), \\\\sigma = $(round(sstd, digits=2))\",\n            size=(800, 600),\n            background_color=\"#7711d708\"\n    )\n    end\n    \nend\n\n@show p\n\n\np = Plot{Plots.GRBackend() n=9}\n\n\n\n\n\n\nThis is just us toying around with the data though, let us now plot the CDF function \\(F\\) and the confidence bounds.\nThose confidence bounds are not very realistic because we treat a variable that’s bounded within \\((0,1)\\) as if it were normally distributed. But it’s a good enough approximation for our purposes.\n\n\nShow the code\n_dfr = dropmissing(sort(dfr, :rel_income))\n\nplot(\n    _dfr.rel_income,\n    _dfr.perc_mean,\n    ribbon=(_dfr.perc_lower_std, _dfr.perc_upper_std),\n    fillalpha=0.2,\n    fillcolor=:blue,\n    label=\"\",\n    xlabel=\"Relative income\",\n    ylabel=L\"Share of population (cdf $\\hat{F}$)\",\n    title=\"Share of population by relative income\",\n    legend=:bottomright,\n    size=(800, 600),\n    ylims=(0, 1.6),\n    xlims=(0, 28),\n    background_color=\"#7711d708\"\n    )\nplot!(\n    _dfr.rel_income,\n    [_dfr.perc_lower_std, _dfr.perc_upper_std],\n    xlims=(0, 28),\n)\n\n\n\n\n\n\nDue to marginalisation across two dimensions, we have a 1D distribution, which we can plot as a curve.\nBut we can clearly see the uncertainty around that curve. Let’s see the quantiles now 🧐\n\n\nShow the code\nplot(\n    _dfr.rel_income,\n    [_dfr.perc_05, _dfr.perc_50, _dfr.perc_95],\n    ribbon=(_dfr.perc_lower_std, _dfr.perc_upper_std),\n    fillalpha=0.2,\n    fillcolor=:blue,\n    label=\"\",\n    xlabel=\"Relative income\",\n    ylabel=L\"Share of population (cdf $\\hat{F}$)\",\n    title=\"Share of population by relative income\",\n    legend=:bottomright,\n    size=(800, 600),\n    ylims=(0, 1.1),\n    xlims=(0, 28),\n    background_color=\"#7711d708\"\n    )"
  },
  {
    "objectID": "posts/small-toy-example/index.html",
    "href": "posts/small-toy-example/index.html",
    "title": "Hierachical modelling in Julia (basics) 🧑‍🏫",
    "section": "",
    "text": "In the previous post, we have seen how to use Julia to estimate a simple univariate model using MLE fitting. In this post, we will see how to use Julia to estimate a hierachical model.\n\n\nYou’ll often see diagrams like these thrown around in the literature:"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to my data notebooks blog !",
    "section": "",
    "text": "This is the first post in my Quarto blog. Welcome ! 👋👋\nThis is “me” in a Picrew avatar below 👇\n\n\n\nThe goal of this small blog is to share my thoughts and ideas about data science, coding, and other things I find interesting. I will also share some of my projects and code snippets. I hope you will enjoy it 🤗.\nI use VSCode and Jupyter to learn Julia bit by bit, and I try to do so using some real world data 😎\n\n\n\nHowever sometimes I might still use ol’ trusty Python 🐍 from time to time (but not R 😅).\n\n\n\n\n\nHere’s a Julia plot of the earth’s GDP growth rate from 1960 to 2019. The data is from the World Bank.\n\n\nShow the code\nusing Plots\nusing DataFrames\nusing CSV\nusing HTTP\nusing ZipFile # for unzipping the file\n\n# download the data\nurl = \"https://api.worldbank.org/v2/en/indicator/NY.GDP.MKTP.KD?downloadformat=csv\";\n\ndownload(url, \"gdp_growth.zip\");\n\n# unzip the file\nr = ZipFile.Reader(\"gdp_growth.zip\");\n\nfor f in r.files\n    if endswith(f.name, \".csv\") && !(startswith(f.name, \"Metadata\"))\n        growthfn = f.name\n        write(open(\"growth.csv\", \"w+\"),read(f, String));\n    end\nend\n\n\n\ndf = DataFrame(CSV.File(\"growth.csv\", header=0, normalizenames=true, silencewarnings=true));\n\n\nworld = df[df[:,1] .== \"World\", :];\n\ngdp = select(world, Not(1:5))\nvecGDP = permutedims(gdp)[:,1]\ndataYears = length(vecGDP)\n\nyears = collect((2021 - dataYears +1):2021);\n\nplot(years, vecGDP, label=\"Total GDP (in 2015 \\$)\", xlabel=\"Year\", ylabel=\"Total GDP (in 2015 \\$)\", legend=:topleft, title=\"Total GDP (in 2015 \\$) from 1968 to 2021\", background_color=\"#7711d708\")\n\n\n\n\n\nCool right ? 😎"
  }
]